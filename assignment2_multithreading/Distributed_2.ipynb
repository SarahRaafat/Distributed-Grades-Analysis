{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9J_wY3uT8ue",
        "outputId": "b6a0bdc8-951c-46b2-d880-e8bda48d8bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset Verification \n",
            "Expected entries: 50\n",
            "Actual entries:   50\n",
            "\n",
            " Entries per Course:\n",
            "  Artificial Intelligence: 8\n",
            "  Machine Learning: 13\n",
            "  Computer Vision: 5\n",
            "  Data Structures: 5\n",
            "  Software Engineering: 10\n",
            "  Cyber Security: 9\n",
            "\n",
            " Entries per Year:\n",
            "   2022: 14\n",
            "   2023: 20\n",
            "   2024: 16\n",
            "\n",
            " Entries per University:\n",
            "  Cambridge University: 7\n",
            "  Harvard University: 14\n",
            "  MIT: 5\n",
            "  Oxford University: 8\n",
            "  Stanford University: 7\n",
            "  UC Berkeley: 9\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Shared dataset where all threads will add entries\n",
        "dataset = []\n",
        "\n",
        "# Binary lock to ensure only one thread writes to the dataset at a time\n",
        "lock = threading.Lock()\n",
        "\n",
        "# Valid values for fields\n",
        "years = [2022, 2023, 2024]\n",
        "courses = set()\n",
        "universities = set()\n",
        "\n",
        "# Step 1: Load unique Course Names and University names from file\n",
        "def load_unique_fields(filename):\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = [p.strip() for p in line.strip().split(',')]\n",
        "            if len(parts) == 4:\n",
        "                _, course, _, university = parts\n",
        "                courses.add(course)\n",
        "                universities.add(university)\n",
        "\n",
        "# Step 2: Thread function to add entries safely\n",
        "def add_entries(thread_id, entries_per_thread):\n",
        "\n",
        "    for _ in range(entries_per_thread):\n",
        "        entry = {\n",
        "            \"Year\": random.choice(years),\n",
        "            \"Course Name\": random.choice(list(courses)),\n",
        "            \"Grade\": random.randint(60, 100),\n",
        "            \"University\": random.choice(list(universities))\n",
        "        }\n",
        "\n",
        "\n",
        "        with lock:\n",
        "            dataset.append(entry)\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "# Step 3: Simulate multiple threads concurrently adding entries\n",
        "def simulate_concurrent_addition(num_threads=10, entries_per_thread=5):\n",
        "\n",
        "    threads = []\n",
        "    for i in range(num_threads):\n",
        "        t = threading.Thread(target=add_entries, args=(i, entries_per_thread))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "# Step 4: Verify consistency and distribution of the final dataset\n",
        "def verify_dataset(num_threads, entries_per_thread):\n",
        "    expected = num_threads * entries_per_thread\n",
        "    print(\" Dataset Verification \")\n",
        "    print(f\"Expected entries: {expected}\")\n",
        "    print(f\"Actual entries:   {len(dataset)}\")\n",
        "\n",
        "    # Count distribution\n",
        "    course_dist = defaultdict(int)\n",
        "    year_dist = defaultdict(int)\n",
        "    university_dist = defaultdict(int)\n",
        "\n",
        "    for entry in dataset:\n",
        "        course_dist[entry[\"Course Name\"]] += 1\n",
        "        year_dist[entry[\"Year\"]] += 1\n",
        "        university_dist[entry[\"University\"]] += 1\n",
        "\n",
        "    print(\"\\n Entries per Course:\")\n",
        "    for course, count in course_dist.items():\n",
        "        print(f\"  {course}: {count}\")\n",
        "\n",
        "    print(\"\\n Entries per Year:\")\n",
        "    for year, count in sorted(year_dist.items()):\n",
        "        print(f\"   {year}: {count}\")\n",
        "\n",
        "    print(\"\\n Entries per University:\")\n",
        "    for uni, count in sorted(university_dist.items()):\n",
        "        print(f\"  {uni}: {count}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    load_unique_fields(\"C:\\Users\\Sara\\OneDrive\\Desktop\\Distributed-Grades-Analysis\\Distributed-Grades-Analysis\\dataset\\coursegrades.txt\")\n",
        "\n",
        "    courses = list(courses)\n",
        "    universities = list(universities)\n",
        "\n",
        "    simulate_concurrent_addition(num_threads=10, entries_per_thread=5)\n",
        "\n",
        "    verify_dataset(num_threads=10, entries_per_thread=5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
