# -*- coding: utf-8 -*-
"""Distributed 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17iIeI6W3wzgJ3XjaZ3R3P8-2N7I-c_YC
"""

import threading
import random
import time
from collections import defaultdict

# Shared dataset where all threads will add entries
dataset = []

# Binary lock to ensure only one thread writes to the dataset at a time
lock = threading.Lock()

# Valid values for fields
years = [2022, 2023, 2024]
courses = set()
universities = set()

# Step 1: Load unique Course Names and University names from file
def load_unique_fields(filename):

    with open(filename, 'r') as file:
        for line in file:
            parts = [p.strip() for p in line.strip().split(',')]
            if len(parts) == 4:
                _, course, _, university = parts
                courses.add(course)
                universities.add(university)

# Step 2: Thread function to add entries safely
def add_entries(thread_id, entries_per_thread):

    for _ in range(entries_per_thread):
        entry = {
            "Year": random.choice(years),
            "Course Name": random.choice(list(courses)),
            "Grade": random.randint(60, 100),
            "University": random.choice(list(universities))
        }


        with lock:
            dataset.append(entry)

        time.sleep(0.01)

# Step 3: Simulate multiple threads concurrently adding entries
def simulate_concurrent_addition(num_threads=10, entries_per_thread=5):

    threads = []
    for i in range(num_threads):
        t = threading.Thread(target=add_entries, args=(i, entries_per_thread))
        threads.append(t)
        t.start()

    # Wait for all threads to finish
    for t in threads:
        t.join()

# Step 4: Verify consistency and distribution of the final dataset
def verify_dataset(num_threads, entries_per_thread):
    expected = num_threads * entries_per_thread
    print(" Dataset Verification ")
    print(f"Expected entries: {expected}")
    print(f"Actual entries:   {len(dataset)}")

    # Count distribution
    course_dist = defaultdict(int)
    year_dist = defaultdict(int)
    university_dist = defaultdict(int)

    for entry in dataset:
        course_dist[entry["Course Name"]] += 1
        year_dist[entry["Year"]] += 1
        university_dist[entry["University"]] += 1

    print("\n Entries per Course:")
    for course, count in course_dist.items():
        print(f"  {course}: {count}")

    print("\n Entries per Year:")
    for year, count in sorted(year_dist.items()):
        print(f"   {year}: {count}")

    print("\n Entries per University:")
    for uni, count in sorted(university_dist.items()):
        print(f"  {uni}: {count}")

# Main execution
if __name__ == "__main__":

    load_unique_fields("C:\Users\Sara\OneDrive\Desktop\Distributed-Grades-Analysis\Distributed-Grades-Analysis\dataset\coursegrades.txt")

    courses = list(courses)
    universities = list(universities)

    simulate_concurrent_addition(num_threads=10, entries_per_thread=5)

    verify_dataset(num_threads=10, entries_per_thread=5)